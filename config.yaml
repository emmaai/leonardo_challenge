weights:
  alpha_embed: 0.45
  beta_caption: 0.20
  gamma_ground: 0.20
  delta_vqa: 0.15
calibration:
  num_random_negatives: 16
grounded_weights:
  entity: 0.5
  attribute: 0.3
  relation: 0.2
str_sim_threshold: 0.6

models:
  embedding:
    target: "matcher:HFCLIPAdapter"
    init:
      model_name: "openai/clip-vit-base-patch32"
      device: "cuda"                # CLIP adapter will .to("cuda")

  captioner:
    target: "transformers:pipeline"
    init:
      task: "image-to-text"
      model: "Salesforce/blip-image-captioning-base"
      device_map: "auto"            # or: device: 0
      torch_dtype: "auto"
      # model_kwargs: { use_safetensors: true }   # optional, recommended

  sts:
    target: "sentence_transformers:SentenceTransformer"
    init:
      model_name_or_path: "all-MiniLM-L6-v2"
      device: "cuda"                # ensures encode runs on GPU

  grounding:
    target: "transformers:pipeline"
    init:
      task: "zero-shot-object-detection"
      model: "google/owlvit-base-patch32"
      device_map: "auto"            # or device: 0
      torch_dtype: "auto"

  vqa:
    target: "transformers:pipeline"
    init:
      task: "visual-question-answering"
      model: "dandelin/vilt-b32-finetuned-vqa"
      device_map: "auto"            # or device: 0
      torch_dtype: "auto"
